from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from huggingface_hub import InferenceClient
import os
from dotenv import load_dotenv
import base64
import io
from PIL import Image
import traceback

load_dotenv()

# ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Client
# ‡πÉ‡∏ä‡πâ Token ‡∏à‡∏≤‡∏Å Render Environment ‡∏´‡∏£‡∏∑‡∏≠ Fallback
hf_token = os.environ.get("HF_TOKEN")
if not hf_token:
    print("‚ö†Ô∏è WARNING: HF_TOKEN missing")

client = InferenceClient(api_key=hf_token)

app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

@app.get("/")
def read_root():
    return {"status": "Hugging Face Server (Optimized) is Live! üöÄ"}

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡πà‡∏≠‡∏£‡∏π‡∏õ (‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏Å‡πâ Error 400) ---
def process_image(image_bytes):
    try:
        # ‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏π‡∏õ‡∏à‡∏≤‡∏Å bytes
        img = Image.open(io.BytesIO(image_bytes))
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô RGB (‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠‡πÑ‡∏ü‡∏•‡πå PNG ‡πÉ‡∏™)
        if img.mode in ('RGBA', 'P'):
            img = img.convert('RGB')
            
        # ‚úÖ ‡∏¢‡πà‡∏≠‡∏£‡∏π‡∏õ: ‡∏ñ‡πâ‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡πÑ‡∏´‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 800px ‡πÉ‡∏´‡πâ‡∏¢‡πà‡∏≠‡∏•‡∏á (AI ‡∏≠‡πà‡∏≤‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Bandwidth)
        max_size = 800
        if max(img.size) > max_size:
            img.thumbnail((max_size, max_size))
            
        # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô Base64 (JPEG Quality 70 ‡∏û‡∏≠)
        buffered = io.BytesIO()
        img.save(buffered, format="JPEG", quality=70)
        img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
        
        return f"data:image/jpeg;base64,{img_str}"
    except Exception as e:
        print(f"‚ö†Ô∏è Image processing failed: {e}")
        # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ‡∏ß‡∏±‡∏î‡∏î‡∏ß‡∏á
        return f"data:image/jpeg;base64,{base64.b64encode(image_bytes).decode('utf-8')}"

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI ‡πÅ‡∏ö‡∏ö‡∏™‡∏π‡πâ‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï ---
def call_huggingface(prompt, image_uri, max_tokens=1000):
    # ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏£‡∏≠‡∏î (‡∏Ç‡∏≠‡∏á‡∏ü‡∏£‡∏µ)
    models = [
        "Qwen/Qwen2-VL-7B-Instruct",       # ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏•‡∏≠‡∏á
        "microsoft/Phi-3.5-vision-instruct", # ‡∏ï‡∏±‡∏ß‡∏™‡∏≥‡∏£‡∏≠‡∏á (‡πÄ‡∏Å‡πà‡∏á‡∏°‡∏≤‡∏Å)
        "meta-llama/Llama-3.2-11B-Vision-Instruct" # ‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå)
    ]
    
    last_error = None
    
    for model_id in models:
        try:
            print(f"üîÑ Trying model: {model_id}...")
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": image_uri}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            completion = client.chat.completions.create(
                model=model_id, 
                messages=messages, 
                max_tokens=max_tokens,
                temperature=0.2
            )
            
            print(f"‚úÖ Success with {model_id}!")
            return completion.choices[0].message.content
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed with {model_id}: {e}")
            last_error = e
            continue
    
    # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏á‡∏ó‡∏∏‡∏Å‡∏ï‡∏±‡∏ß ‡πÉ‡∏´‡πâ‡πÇ‡∏¢‡∏ô Error ‡∏à‡∏£‡∏¥‡∏á‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
    raise last_error

# --- Analyze Endpoint ---
@app.post("/analyze")
async def analyze_ui(
    file: UploadFile = File(...), 
    country: str = Form(...), 
    context: str = Form(...)
):
    print(f"üì• Analyze: {country}")
    try:
        contents = await file.read()
        # ‚úÖ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡πà‡∏≠‡∏£‡∏π‡∏õ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á
        image_uri = process_image(contents)
        
        prompt = f"""
        Act as a UX/UI Expert. Analyze this UI for {country} culture.
        Context: {context}.
        Output ONLY raw HTML with: Score (0-100), Critical Issues, and Suggestions.
        Do NOT use markdown.
        """
        
        result = call_huggingface(prompt, image_uri)
        return {"result": result.replace("```html", "").replace("```", "").strip()}

    except Exception as e:
        print("‚ùå Error:", e)
        traceback.print_exc()
        # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ Error ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ô‡∏µ‡∏¢‡∏ô‡πÜ ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏û‡∏±‡∏á
        return {"result": f"<div style='color:red'><h3>AI Busy/Error</h3><p>{str(e)}</p></div>"}

# --- Fix Endpoint ---
@app.post("/fix")
async def fix_ui(
    file: UploadFile = File(...), 
    country: str = Form(...), 
    context: str = Form(...),
    description: str = Form(""), 
    width: str = Form("375"),    
    height: str = Form("812"),
    keep_layout: str = Form("false")
):
    try:
        contents = await file.read()
        image_uri = process_image(contents)
        
        prompt = f"""
        Create SVG wireframe for {country}. {width}x{height}.
        Output ONLY raw SVG. Start with <svg.
        """
        
        svg = call_huggingface(prompt, image_uri, max_tokens=2000)
        
        clean_svg = svg.replace("```svg", "").replace("```xml", "").replace("```", "").strip()
        if "<svg" in clean_svg: clean_svg = clean_svg[clean_svg.find("<svg"):]
        if "</svg>" in clean_svg: clean_svg = clean_svg[:clean_svg.find("</svg>")+6]
        
        return {"svg": clean_svg}

    except Exception as e:
        print("‚ùå Error:", e)
        return {"svg": ""}